{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "from tensorflow import keras \n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Bidirectional,Concatenate\n",
    "from keras import Model\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import indian\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwd = stopwords.words('english')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "## Processing the input data\n",
    "def clean(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub(',', ' ', text)\n",
    "    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "    text = re.sub('”', '', text)\n",
    "    text = re.sub('“', '', text)\n",
    "    text = re.sub('’', '', text)\n",
    "    text = re.sub('‘', '', text)\n",
    "    text = re.sub('\\\\n', ' ', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    lst = nltk.word_tokenize(text)\n",
    "    return lst\n",
    "\n",
    "\n",
    "\n",
    "def stopwo(lst):\n",
    "    for word in lst:\n",
    "        if word in stopwd:\n",
    "            lst.remove(word)\n",
    "    return lst\n",
    "    \n",
    "\n",
    "\n",
    "def merge(english):\n",
    "    eng_merged=[]\n",
    "    sentence=\"\"\n",
    "    for word in english:\n",
    "        sentence=sentence+word+\" \"\n",
    "    eng_merged.append(sentence.strip())\n",
    "    return eng_merged\n",
    "\n",
    "\n",
    "with open('eng_tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer_eng = pickle.load(file)\n",
    "\n",
    "eng_index=tokenizer_eng.word_index\n",
    "\n",
    "with open('hindi_tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer_hindi = pickle.load(file)\n",
    "\n",
    "hindi_index=tokenizer_hindi.word_index\n",
    "\n",
    "def process_input(text):\n",
    "    text = clean(text)\n",
    "    text = tokenize_text(text)\n",
    "    text = stopwo(text)\n",
    "    text = merge(text)\n",
    "    print(text)\n",
    "    text = tokenizer_eng.texts_to_sequences(text)\n",
    "    max_len_english = 260\n",
    "    padded_input_sequences = pad_sequences(text, maxlen = max_len_english, padding='post')\n",
    "    # print(padded_input_sequences)\n",
    "    return padded_input_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## defining model\n",
    "def encoder_decoder_model(vocab_size_input, vocab_size_output, max_seq_length_input, max_seq_length_output, embedding_dim, hidden_units):\n",
    "    # Define encoder input layer\n",
    "    encoder_inputs = Input(shape=(max_seq_length_input,))\n",
    "    \n",
    "    # Define encoder embedding layer\n",
    "    encoder_embedding = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim)(encoder_inputs)\n",
    "    \n",
    "    # Define encoder LSTM layer\n",
    "    encoder_lstm = Bidirectional(LSTM(hidden_units, return_state=True))\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "    \n",
    "    # Concatenate forward and backward states\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Define decoder input layer\n",
    "    decoder_inputs = Input(shape=(max_seq_length_output-1,))\n",
    "    \n",
    "    # Define decoder embedding layer\n",
    "    decoder_embedding = Embedding(input_dim=vocab_size_output, output_dim=embedding_dim)(decoder_inputs)\n",
    "    \n",
    "    # Define decoder LSTM layer with initial state set to encoder states\n",
    "    decoder_lstm = LSTM(hidden_units * 2, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    \n",
    "    # Define decoder output layer\n",
    "    decoder_dense = Dense(vocab_size_output, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    # model.summary()\n",
    "    return model, encoder_inputs, encoder_states, decoder_inputs, decoder_embedding, decoder_lstm, decoder_dense\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Defining model input and output data parameters\n",
    "word_count_eng=72459\n",
    "word_count_hindi= 76219\n",
    "max_len_english=260\n",
    "max_len_hindi= 418\n",
    "embedding_dim = 100  \n",
    "hidden_units = 256  \n",
    "batch_size = 64\n",
    "\n",
    "model, encoder_inputs, encoder_states, decoder_inputs, decoder_embedding, decoder_lstm, decoder_dense= encoder_decoder_model(word_count_eng, word_count_hindi, max_len_english, max_len_hindi, embedding_dim, hidden_units)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "with open('weight_new_non_null.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "model.set_weights(data)\n",
    "\n",
    "# Define the inference model for the encoder\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define the initial state for the decoder\n",
    "decoder_state_input_h = Input(shape=(hidden_units * 2,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units * 2,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Define the decoder LSTM layer\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder output layer\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the inference model for the decoder\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_predicted_sentence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = hindi_index['sos']\n",
    "\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        print(output_tokens[0,-1,:])\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # print(\"output\",sampled_token_index)\n",
    "        if sampled_token_index==0:\n",
    "          break\n",
    "        else:   \n",
    "         # convert max index number to hindi word\n",
    "         for key, value in tokenizer_hindi.word_index.items():\n",
    "            if value == sampled_token_index:\n",
    "                # print(f\"The key for value 31723 is '{key}'\")\n",
    "                sampled_char=key\n",
    "                break\n",
    "        #  sampled_char = hindi_index[sampled_token_index]\n",
    "        # aapend it to decoded sentence\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "        \n",
    "        # Exit condition: either hit max length or find stop token.\n",
    "        if (sampled_char == 'eos' or len(decoded_sentence) >= 417):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king deprived his power']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "[5.4208166e-04 1.3021423e-06 5.1110578e-03 ... 1.6247916e-06 1.3782288e-06\n",
      " 1.4326365e-06]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "[8.8704709e-04 1.7582483e-06 1.3272254e-02 ... 2.0187197e-06 1.8113226e-06\n",
      " 1.8537996e-06]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "[4.3206787e-04 3.4083912e-06 5.9272684e-03 ... 3.7774755e-06 3.4457014e-06\n",
      " 3.5448352e-06]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "[3.2188004e-04 4.9012565e-06 3.7787559e-03 ... 5.2971259e-06 4.9225537e-06\n",
      " 5.0299864e-06]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "[3.2857017e-04 5.2614946e-06 3.8524908e-03 ... 5.6446202e-06 5.3043213e-06\n",
      " 5.4049742e-06]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "[3.7826845e-04 5.1313673e-06 5.0037522e-03 ... 5.5046153e-06 5.2295909e-06\n",
      " 5.3059152e-06]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "[4.4151206e-04 4.8618676e-06 6.4163567e-03 ... 5.2329297e-06 4.9966693e-06\n",
      " 5.0600252e-06]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "[4.9522455e-04 4.6103364e-06 7.4928170e-03 ... 4.9828909e-06 4.7566009e-06\n",
      " 4.8196357e-06]\n",
      " इस में में में में में में\n"
     ]
    }
   ],
   "source": [
    "## Sentence to predict\n",
    "text = 'The king was deprived of his power.'\n",
    "test = process_input(text)\n",
    "prediction = get_predicted_sentence(test)[:-4]\n",
    "if(len(prediction)==0):\n",
    "    print('Cannot do it as of now!!')\n",
    "else:\n",
    "    print(prediction)\n",
    "# print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_flow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
